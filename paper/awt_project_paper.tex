\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithm,algorithmic}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\definecolor{lightgray}{rgb}{.9,.9,.9}
\definecolor{darkgray}{rgb}{.4,.4,.4}
\definecolor{purple}{rgb}{0.65, 0.12, 0.82}

\lstdefinelanguage{JavaScript}{
  keywords={typeof, new, true, false, catch, function, return, null, catch, switch, var, if, in, while, do, else, case, break},
  keywordstyle=\color{blue}\bfseries,
  ndkeywords={class, export, boolean, throw, implements, import, this},
  ndkeywordstyle=\color{darkgray}\bfseries,
  identifierstyle=\color{black},
  sensitive=false,
  comment=[l]{//},
  morecomment=[s]{/*}{*/},
  commentstyle=\color{purple}\ttfamily,
  stringstyle=\color{red}\ttfamily,
  morestring=[b]',
  morestring=[b]"
}

\lstset{
   language=JavaScript,
   backgroundcolor=\color{lightgray},
   extendedchars=true,
   % frame=single,
   basicstyle=\small\ttfamily,
   showstringspaces=false,
   showspaces=false,
   tabsize=2,
   breaklines=true,
   showtabs=false,
   captionpos=b
}
\begin{document}

\title{User Interface for Video and Image Stitching}

\author{
\IEEEauthorblockN{1\textsuperscript{st} Nguyen, Trung Han}
\IEEEauthorblockA{\textit{Open Distributed Systems} \\
\textit{Technische Universität Berlin}\\
Berlin, Germnay \\
trung.h.nguyen@campus.tu-berlin.de
}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Tran, Minh Duc}
\IEEEauthorblockA{\textit{Open Distributed Systems} \\
\textit{Technische Universität Berlin}\\
Berlin, Germnay \\
minh.d.tran@campus.tu-berlin.de
}
\and
\IEEEauthorblockN{3\textsuperscript{rd} Tran, Nhat Duc}
\IEEEauthorblockA{\textit{Open Distributed Systems} \\
\textit{Technische Universität Berlin}\\
Berlin, Germnay \\
nhat.d.tran@campus.tu-berlin.de
}
}

\maketitle

\begin{abstract}
This document is a model and instructions for \LaTeX.
This and the IEEEtran.cls file define the components of your paper [title, text, heads, etc.]. *CRITICAL: Do Not Use Symbols, Special Characters, Footnotes, 
or Math in Paper Title or Abstract.
\end{abstract}

\begin{IEEEkeywords}
web UI, video and image stitching, MPD, DASH, Amazon Simple Queue Service (SQS)
\end{IEEEkeywords}

\section{Introduction}
With the recent progresses in cloud computing in the areas of cost-efficiency, scalability etc., more and more software applications are shifted from local computers to external servers.
While in the past software like audio or graphics editor tools had to be installed locally due to high processing demands, nowadays mentioned applications can be provided over the internet.

The Fraunhofer FOKUS\footnote{https://www.fokus.fraunhofer.de/} \textit{(Fraunhofer Institute for Open Communication Systems)} is a research unit of the Fraunhofer Society\footnote{https://www.fraunhofer.de/} and is concerned with applied research and development of applications in the field of Information and Communications Technology.
Among the projects of FOKUS is the development of such a service for video and image stitching, similar to well-known traditional software like Window's Movie Maker\footnote{http://windows.microsoft.com/en-us/windows/get-movie-maker-download}, but in this case over the internet.

Using media URLs from any host or cloud provider, the backend creates an MPD file (manifest file for adaptive media streaming\cite{Sodagar}) according to a given stitching configuration.
Amazon's Simple Queue Service\footnote{http://aws.amazon.com/sqs/} (SQS) serves as communication channel between frontend and service backend.

As of now FOKUS has only implemented a stitching backend. A proper user interface for seamless interaction with the backend is still missing.
In this work we implemented a web UI\footnote{Code available at https://github.com/SmokyDesperado/moveditor} for that very purpose.

In particular this inlcudes:
\textbf{(II)} A glimpse into related work and what elevates this work.
\textbf{(III)} and \textbf{(IV)} The Architecture and implementation, in which non-trivial steps are described and important decisions made while developing the UI are explained.
\textbf{(V)} An evaluation of the approach taken for implementing the user interface as well as a discussion of its strong points but also its flaws.
\textbf{(VI)} A conclusion to this work and suggestions for future work.

\section{Related Work}
\textbf{ToDo: Han} \\
List online/offline video stitching tools, differences and what makes our web app unique...

\section{Architecture \& Used Technologies}
\textbf{ToDo: Han} \\
Web UI communicates with FH backend via SQS; using AngularJS, AWS SDK, clouds...

\section{Design \& Implementation}
List some features etc. but this section will only focus on main aspects of our web ui implementation and why things were done that way, bower (what?, why?, how?) + setup, ...

\subsection{General}
UI primarily for Desktop, save session via .txt export...

\newpage

\subsection{Content}
\textbf{ToDo: Minh} \\
% % general\\
The content area contains all the base videos, images and audios which are used in the app to stitch and create a new video. It is devided into two segments, the menu at the top and below the actual content area with all the materials. New content materials are loaded and added to the area via a public accessible URL. Meta information, like the thumbnail or content length, will be loaded and added to visible representation of the material. Content materials can be dragged and moved around to add them in the timeline or delete them. The user also has the possibility to save and load the current session and watch or hear the content material if the content of the material is unknown or forgotten.

% content object\\
For displaying the materials in the content area the contentList object is used and binded to the view. This object is a simple JavaScript dictionary object which consist of a random generated contentId as key and an instance of the JavaScript class Content as value. The dictionary data type was chosen to eneabled the possibility of having direct access to the object of the contentList by only knowing the contentId. It is possible to pass the contentId of the materias from one service to another and only access the needed material properties right before using.

The instances of the Content class consist of a name, a type, a length, a url, a mpd and a active property. The name property is empty at the beginning and a user can change the name after the content materia is loaded. The url property is is the public accessible URL of the video, audio or image which is used to add the material. It is not possible to changed it afterwards. Each material URL is unique for the contentList. Dublicated material URLs will not be added to the contentList. As a result that only the URL is validated and not the content of the material, a duplicated content material can be added to the list by using the same content but with different URLs. New content is added to the contentList by adding a video, image or audio to a cloud or web service with a public accessible URL and the adding this url to the input field of the content area menu.\\
\\
== bild vom content material ==\\
\\
The meta information of the content material like type and the length are loaded after adding the content to the list via the HTML Audio/Video Events. For a better visualisation an image of the middle of the material video is added as a thmubnail while loading the meta information. The image itself is used for image typed content and for audio typed materials a pre defined image is used. Farther an icon on the lower right corner of the content also indicates the typed of the content material.

The active property of a material is used to keep track of amounts of the material used in the timeline. Each material adding to the timeline increase the active counter and each remove from the timeline decrease the counter. Unless the active property equals 0 it is not possible to delete the content material. The last property the mpd property is not editable by the user. It is used to store the mpd URL after the segmentation process of the backend and then used for the stitching request.

The drag and drop functionality to add content materials into the timeline is enabled through the AngularJS module AngularHammer(1) by Ryan Mullins. The already existing touch, mouse and pointerEvents recognition JavaScript library HAMMER.JS (2) was made into a AngularJS module for a simple usage. The three important features of this module were the panStart, panMove and panEnd events. PanMove provided functionalites to recognise the mouse events and manipulate the DOM element while panStart and panEnd provided manupulation right before and after panMove is executed. So it was possible to initialise needed helper variables (i.e. activate attributes to react while moving) or saving the current app state after moving elements.

Another aspect of the content area menu is saving and loading working sessions. Saving can save the current state of the content and timeline area and loading wiill recreate the state of a saved working session from a file. Based on the used data structure of the app, the saving file is a .txt file containing one JSON object. The save process will collect all needed information of the different parts and create the saving file and loading will read the file and recreates saved state of the app. Because of the simplicity of the saved file and the absent of validation while loading, a use can manupulate the information and the state of the app.

For checking the loaded content, the user can double click on the specific material. This opens up a pop up viewer / player showing image typed materials or playing video or audio typed materials. The double click is handled by AngularHammer and will create an overlay on the app. Normally the overlay is hided in the background and jumps to the foreground when activated. The dynamically created and added content of the overlay depends on the type, whether it is a audio player, a video player or just a image, of the material. By clicking outside the overlay content, the content will be pause (only for audio and video player), destroyed and the overlay will hide in the background again.\\
\\
(1) - https://github.com/RyanMullins/angular-hammer\\
(2) - http://hammerjs.github.io/getting-started\\
\\
% ========\\
% X - dictionary and reason\\
% \\
% X - add new content\\
% X - loading meta info\\
% X - URL input for adding new contents from web/clouds... \\
% \\
% X - drag and drop features\\
% \\
% X - saving and loading\\
% \\
% O - preview content\\

\subsection{Timeline Area}
\textbf{ToDo: Minh} \\
most controls using hammer JS...

\newpage

\subsection{Preview Player}
The preview player emulates the stitching result of the service backend without actually sending the necessary requests.
In doing so, imperfections in the stitching configurations can be detected and corrected via real-time feedback.
Users are not impelled to wait for the actual stitching result, thus saving them time.
\\
\subsubsection{Preview Player - Features}
The implemented preview player supports all common controls of a usual browser media player.
These include: a ``play/pause''-button, a ``time-position''-slider and a ``volume''-slider.
On top of that, a ``restart''-button, a ``loop-play''-toggle and a ``play-in-range''-slider were added.
For the latter the \textit{noUiSlider}\footnote{https://refreshless.com/nouislider/} library is utilized.
Functionally, the ``play-in-range''-slider marks a time range in which the ``time-position''-slider is contraint.
Almost all of the listed controls can be operated via shortkeys\footnote{manual/wiki} making the web UI feel more like a native application.
\\
\subsubsection{Preview Playback - Logic}
For audio and image preview playback it is sufficient to only have one HTML audio, respectively image, element and change its sources in real-time while playing.
However, this method does not work for all video sources, e.g. cloud provider URLs generally take more time for the initial loading which leads to stuttering, especially right after swapping video sources.
Therefore, per video chunk the implemented player creates one dedicated HTML video element with preloaded source.
While playing, it then only manipulates the z-indexes of the video elements and stops or starts them accordingly (see Algorithm 1) resulting in a smoother playback and minimized buffering time.

As optimization, whenever a new video chunk is added to the timeline area, the preview player is signalled to inspect whether there already exists an HTML video element referencing the same source.
If there is none, then one will be created.
Analogously, if a video chunk is removed from the timeline, then its corresponding HTML video element is deleted if there is no other chunk requiring the same source.
\begin{algorithm}[H]
\caption{Preview playback loop, simplified}
	\begin{algorithmic}[1]
		\STATE cc $\gets$ current chunk
		\STATE pc $\gets$ previous chunk
		\IF {cc $\ne$ pc $\AND$ (pc.type $=$ video $\OR$ pc.type $=$ audio)}
		\STATE pcHTML $\gets$ corresponding HTML video/audio element
		\STATE pause pcHTML
		\ENDIF
		\IF {time $=$ timeline-end $\OR$ time $=$ range-end}
		\IF {loop play active}
		\STATE go to range-start
		\ELSE
		\STATE pause player
		\RETURN
		\ENDIF
		\ENDIF
		\STATE bring current video/image to the front via z-index
		\IF {cc.type $=$ video $\OR$ cc.type $=$ audio}
		\STATE ccHTML $\gets$ corresponding HTML video/audio element
		\STATE calculate ccHTML offset position
		\STATE mute ccHTML if necessary
		\STATE play ccHTML
		\ENDIF
		\STATE pc $\gets$ cc
		\STATE time $\gets$ time counter + 100 ms
		\STATE diff $\gets$ (real-time-passed - time)
		\STATE repeat Algorithm 1 in (100 ms - diff)
	\end{algorithmic}
\end{algorithm}
Due to the nature of Javascript, Algorithm 1 had to be seperated into two functions code-wise or else the parallel updated time counter would start one loop ahead of the preview player resulting in both units not being in sync.
To keep the time counter even more precise, a self-adjustment calculation was introduced. By tracking the real-world time passed and matching that with the internal player time, it can be ensured that the preview playback loop is almost always repeated accurately every 100 milliseconds.

\subsection{SQS}
Once the user is satisfied with the constructed timeline chunks, the whole stitching procedure can be initiated by pressing the ``stitching''-button.
The stitching process consists of two phases: 1.) A segmentation phase and 2.) a stitching phase.

For communicating, the stitching backend operates over two seperate SQS queues.
From the web client perspective, one queue is for sending requests, the other is needed for receiving results.
Both are used for segmentation as well as for stitching requests.
The request type is determined through the message body by the service backend.
Further, each message body possesses a user defined \textit{jobID} which helps users to identify their messages in the queue for results.
As with other libraries utilized in the developed user interface, functionalities of Amazon's SQS were made accessible by incorporating the official AWS SDK\footnote{https://github.com/aws/aws-sdk-js\#using-bower} for JavaScript via Bower.
\begin{figure}[H]
\begin{lstlisting}
var sqs = new AWS.SQS({
  "accessKeyId": sqsAccessKeyId,
  "secretAccessKey": sqsSecretAccessKey,
  "region": sqsRegion
});
\end{lstlisting}
\caption{Setting up an SQS instance in AngularJS}
\end{figure}
At the beginning, the condition of the timeline and content area at process starting time are temporarily preserved.
By working on those data, accidental user changes and edits in the meantime don't affect the stitching procedure.
To avoid further problems, it is not possible to send another stitching request until the whole process is completed.
\\
\subsubsection{Segmentation Phase}
In this context, segmentation means creating an MPD file based on an audio, image or video file, requiring only a public accessible resource URL.
Theoretically, since segmentation is applied to files in full length and not single chunks, this step could be executed as soon as a source is added to the content area or when a content is added to the timeline.
However, the user has to wait for the stitching process either way and not every resource might be used in the end which would then lead to spamming and wasting storage of the FOKUS backend.
Also worth mentioning is that for segmenting images a length must be specified priorly which is only possible after actually working on the timeline.

During phase 1, chunks in the timeline are fragmented in chronological order.
A segmentation request message to a chunk is created and put on the request queue (see Fig. 2).
\begin{figure}[H]
\begin{lstlisting}
var msg = {
  jobID: segmentationId,
  S3URL: chunkUrl,
  encodingprofile: "default",
  requestEnqueueTime: +new Date()
};

var params = {
  MessageBody: JSON.stringify(msg),
  QueueUrl: requestQueueURL,
  MessageGroupId: 'myGroupId'
};

sqs.sendMessage(params, function(err, data) {
  if (err) {
    console.log('ERROR: ', err);
  } else {
    console.log("send request successfully");
  }
});
\end{lstlisting}
\caption{Example request for segmenting a video}
\end{figure}
After the segmentation request was send, the result queue will be polled for via a recursive receiving function.
The reason for this lies in the AWS SDK, which only allows the client to receive at most the 10 oldest messages on the result queue.
This in turn means that if 10 or more messages of other users are occupying the queue first, then the client has to wait until those are removed by their target user.

The implemented approach polls for the 10 messages at a time and only considers those targeting the user via identifying the \textit{jobID}.
Are among those messages no result messages, a progress per chunk segmentation is shown and the next 10 are polled for.
In case one segmentation is done, a message contains an MPD URL, which is saved temporarily and the next chunk will be segmented, but only if its corresponding source URL has not been segmented yet.
Either way, those messages are then removed from the queue by its proper recipient so that the next results in the FIFO queue can be received.
\\
\subsubsection{Stitching Phase}
Are all chunks segmented successfully, the stitching phase starts.
By sewing MPDs together according to a specified configuration, a new video MPD is generated.
Except the message to be transmitted, sending and receiving a stitching request are analog to that of a segmentation request.
\begin{figure}[H]
\begin{lstlisting}
var configStitching = {
  "LOG_LEVEL": "info",
  "server": {
    "url": "http://localhost:8090",
    "staticFolder": "adstitcher-srv/public/mpds",
    "dashEndpoint": "/Users/fr/Documents/adstitcher-srv/public/mpds/"
  },
  "content": [
    {
      "type":"video",
      "begin":0.8,
      "end":5.5,
      "offset":0,
      "mute":false,
      "hide":false,
      "url":"https://s3.eu-central-1.amazonaws.com/wesualizesegment/exampleVideo1/Manifest.mpd"
    },
    {
      "type":"video",
      "begin":6.3,
      "end":10.0,
      "offset":3.7,
      "mute":false,
      "hide":false,
      "url":"https://s3.eu-central-1.amazonaws.com/wesualizesegment/exampleVideo2/Manifest.mpd"
    }
  ]
}

var msg = {
  jobID: stitchingId,
  config: configStitching,
  requestEnqueueTime: +new Date()
};
\end{lstlisting}
\caption{Example message of a stitching request}
\end{figure}
While the segmentation works perfectly, the stitching request results in an error (\textit{errorCode 11: "something wrong with the dashEndpoint in the config. Please check the input config."}).
Since there was no documentation of the stitching backend and because its developers were not always contactable, this issue could not be resolved in this work.
\\
\subsubsection{Problems when receiving messages}
During tests it became apparent, that not every polling call resulted in the client receiving messages.
Despite that, when only one client is polling for the FIFO result queue at a time, it works perfectly.
As soon as more users attempt to receive messages, the whole process is slowed down massively.
It may be that the concurrent pollings from different clients are interferring with each other so that the probability of not receiving any messages is increased a lot.
This way, the FIFO queue can not make any or only very slow progresses.
On the other hand it might also be that the SQS queues are deployed on budget servers that are not able to actually handle multiple requests simultaneously.

Even worse, when many clients are polling for results, and some of these abort this process, then that might lead to stagnation of the result queue, whereby no active user can receive any more messages.

In this prototype version of the UI, a ``purge\&restart''-button was implemented as a safety measure for now.
This button will remove all messages on the queue and restart the FOKUS backend server, although this is no longterm solution to menttioned problems.

\section{Discussion}
What works, what does not? Shortcoming?\\

- SQS and FH backend are shit; a intermediary backend managing the request (msg timeouts etc...) and providing an API would be better; suitability for use with regard to user-friendliness, performance, robustness... \\
- server und queues nicht robust \\
Shortcomings: quantization 100ms -$>$ video ende abgeschnitten, 10ms -$>$ schwächere cpu schaffen nicht\\

\section{Conclusion}
webUI great, SQS and FH backend are shit...
\\\\
\textit{Future Work}. open features and improvements... \\
- undo/redo auch für content \\
- timeline delete: change position slider \\
- timeline position- und range-slider als controls nutzen \\
- whitelist check und url acceptance verbessern \\
- chunk add time display als hover \\
- sav-file correctness check \\
- info page \\
- SQS strategie wenn client stirbt: intermediary backend, create session savefile before sending process und notfalls von hier weitermachen \\

\begin{thebibliography}{00}
\bibitem{Sodagar} I. Sodagar, ``The MPEG-DASH Standard for Multimedia Streaming Over the Internet'', in IEEE MultiMedia, vol. 18, no. 4, pp. 62-67, Washington, D.C.: IEEE Computer Society, October 2011. doi:10.1109/MMUL.2011.71
\end{thebibliography}

\end{document}
