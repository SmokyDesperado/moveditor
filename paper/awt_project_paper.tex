\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithm,algorithmic}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\definecolor{lightgray}{rgb}{.9,.9,.9}
\definecolor{darkgray}{rgb}{.4,.4,.4}
\definecolor{purple}{rgb}{0.65, 0.12, 0.82}

\lstdefinelanguage{JavaScript}{
  keywords={typeof, new, true, false, catch, function, return, null, catch, switch, var, if, in, while, do, else, case, break},
  keywordstyle=\color{blue}\bfseries,
  ndkeywords={class, export, boolean, throw, implements, import, this},
  ndkeywordstyle=\color{darkgray}\bfseries,
  identifierstyle=\color{black},
  sensitive=false,
  comment=[l]{//},
  morecomment=[s]{/*}{*/},
  commentstyle=\color{purple}\ttfamily,
  stringstyle=\color{red}\ttfamily,
  morestring=[b]',
  morestring=[b]"
}

\lstset{
   language=JavaScript,
   backgroundcolor=\color{lightgray},
   extendedchars=true,
   % frame=single,
   basicstyle=\small\ttfamily,
   showstringspaces=false,
   showspaces=false,
   tabsize=2,
   breaklines=true,
   showtabs=false,
   captionpos=b
}
\begin{document}

\title{User Interface for Video and Image Stitching}

\author{
\IEEEauthorblockN{1\textsuperscript{st} Nguyen, Trung Han}
\IEEEauthorblockA{\textit{Open Distributed Systems} \\
\textit{Technische Universität Berlin}\\
Berlin, Germnay \\
trung.h.nguyen@campus.tu-berlin.de
}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Tran, Minh Duc}
\IEEEauthorblockA{\textit{Open Distributed Systems} \\
\textit{Technische Universität Berlin}\\
Berlin, Germnay \\
minh.d.tran@campus.tu-berlin.de
}
\and
\IEEEauthorblockN{3\textsuperscript{rd} Tran, Nhat Duc}
\IEEEauthorblockA{\textit{Open Distributed Systems} \\
\textit{Technische Universität Berlin}\\
Berlin, Germnay \\
nhat.d.tran@campus.tu-berlin.de
}
}

\maketitle

\begin{abstract}
Due to progresses and benefits of cloud computing in the recent years, it has become easier than ever for developers to deploy any kind of services over the internet.
Using and flexibly provisioning of resources from a pool shared by third-party organizations for reasonable prices, developers are no longer troubled with IT-infrastructural maintenance issues.
Fraunhofer FOKUS developed a backend for video and image stitching relying on such a cloud computing solution in forms of Amazon's Simple Queue Service (SQS).
Utilizing that tool however has been rather difficult because a proper frontend is still missing.
For that purpose, we implemented a corresponding web user interface.
This work presents all relevant steps of its realization and explains the most important conceptual decisions that were made.
While in itself, the implemented user interface works flawlessly, the communication with the stitching backend via SQS ran into problems.
Because of an unforeseeable error and the developers of the FOKUS backend not always being contactable, the final stitching result is not available at the moment.
Furthermore, whenever two or more clientS concurrently poll on the result queue, the overall process is decelerated greatly.
The reason for that might either lie in the long polling algorithm, a possibly budget server deploying the queues or even AWS' own receiving function.
An intermediary backend between the UI and the queues is proposed as one possible solution so that the queues only need to handle one requesting unit at a time.
In spite of mentioned deficiencies, the implemented prototype still qualifies as a fully functional user interface and forms the basis for a frontend of the existing stitching backend by Fraunhofer FOKUS.
\end{abstract}

\begin{IEEEkeywords}
web UI, video and image stitching, MPD, cloud computing, Amazon Simple Queue Service (SQS)
\end{IEEEkeywords}

\section{Introduction}
With the recent advancements in cloud computing in the areas of cost-efficiency, flexibility, scalability etc., it is easier than ever for developers to provide on-demand software applications over the internet.
They no longer require large sums to deploy their services as well as don't need to be troubled with infrastructural maintenance issues, such as under- or overprovisioning of resources, if an offered service exceeds or falls below expectations \cite{Fox}.

The Fraunhofer FOKUS\footnote{https://www.fokus.fraunhofer.de/} \textit{(Fraunhofer Institute for Open Communication Systems)} is a research unit of the Fraunhofer Society\footnote{https://www.fraunhofer.de/} and is concerned with applied research and development of applications in the field of Information and Communications Technology (ICT).
Among the projects of FOKUS is the development of a service for video and image stitching, similar to well-known traditional software like Windows Movie Maker\footnote{http://windows.microsoft.com/en-us/windows/get-movie-maker-download}, but in this case over the internet relying on cloud computing solutions.

Using media URLs from any host or cloud provider, the backend creates an MPD file (manifest file for adaptive media streaming \cite{Sodagar}) according to a given stitching configuration.
Amazon's Simple Queue Service\footnote{http://aws.amazon.com/sqs/} (SQS) serves as communication channel between frontend and service backend.
SQS - being a fast, reliable and scalable message queuing service - makes it simple and cost-effective to decouple the components of a cloud application since the user can offload the administrative burden of operating and scaling, while paying only for what is really needed \cite{VS}.

As of now FOKUS only provides a stitching backend. A proper user interface for seamless interaction with the backend is still missing.
In this work we implemented corresponding a web UI\footnote{Code available at https://github.com/SmokyDesperado/moveditor} for that very purpose.
In particular this inlcudes:
\textbf{(II)} A glimpse into related work and what elevates this work.
\textbf{(III)} and \textbf{(IV)} The Architecture and implementation, in which non-trivial steps are described and important decisions made while developing the UI are explained.
\textbf{(V)} An evaluation of the approach that was taken for implementing the user interface as well as a discussion of its strong points but also its flaws.
\textbf{(VI)} A conclusion to this work and suggestions for future work.

\section{Related Work}
%List online/offline video stitching tools, differences and what makes our web app unique...
There are various programs, on- and offline, available to edit videos, images and audios into one file.
The majority of them are video editing tools.

For example, \textit{Microsoft Fotos}\footnote{https://www.microsoft.com/de-de/p/microsoft-fotos/9wzdncrfjbh4}, the successor of Windows Movie Maker and one of the simplest video editing tools, is able to stitch videos, images and audios.
Though, this software can not overlap the video and picture track with soundtrack.

\textit{Adobe Premiere Pro}\footnote{https://www.adobe.com/products/premiere.html} is the industry-leading video editing software.
It also supports functions to adjust the length of different media files as well as to stitch them together.
It is however not possible to export an Adobe Premiere Pro project as an MPD file for adaptive streaming.

An online solution for video editing is \textit{Movie Maker Online}\footnote{http://moviemakeronline.com/}.
This Editor has every editing functionalities that we need but it also does not support exporting the finished product as an MPD file.

\section{Architecture \& Used Technologies}
Frauenhofer FOKUS provides an already existing stitching backend.
Not much is known except that requests can only be forwarded to that backend via Amazon's Simple Queue Service.
The only left component is the frontend we needed to implement.
So the architecture we use is similar to a classic frontend-backend architecture, but modified due to the constraint of using SQS.

The frontend utilizes the AngularJS framework and communicates with the back-end via SQS by using the AWS SDK\footnote{https://aws.amazon.com/de/tools/} \textit{(Amazon Web Services Software Development Kit)}.

The workflow should be as follows:
1.) Users upload media files (images, videos, audios) to any cloud service or host (e.g. Google Drive, Dropbox etc.) so that these files can be accessed through a publicly accessible URL.
2.) This URL can then be added to the implemented user interface.
3.) On the UI, users edit video, image and audio files and finish their work by sending requests to the back-end via the queues.
4.) At first, all used media files will be segmented.
5.) The requests are pushed to the request queue and then fetched by the FOKUS backend.
6.) The backend creates corresponding MPDs and the results in form of URLs are put on the result queue by the backend for the user to obtain.
7.) These MPD URLs are used to create a stitching configuration, which will again be forwarded to the backend via the queues.
8.) After that process is finished, the user gets the final stitching result.

Figure \ref{fig:architecture} shows the architecture and the used technologies.
It also points out the workflow.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.27]{architecture.png}
	\caption{Architecture and where which technologies are used.}
	\label{fig:architecture}
\end{figure}

\section{Design \& Implementation}
This section mainly focusses on the implementation of the user interface for image and video stitching.
The realization of major components is presented and important decisions will be explained to give a better understanding of this application.

\subsection{General}
For the implementation of the user interface, various libraries were used.
Its main structure is generated via AngularJS project scaffolding by Yeoman\footnote{http://yeoman.io/learning/}.
In doing so, a ready-to-use AngularJS project is created with provided directory structure, default templates and pre-installed components required for running the application as well as packages to boost the development process.

Node.js\footnote{https://nodejs.org/en/} serves as the main run-time enviroment for the development process and is used to create the server while developing.
New modules to help the development can be installed via the package manager npm\footnote{https://www.npmjs.com/}.
The used packages are saved in the file \textit{package.json} and can be managed from there, e.g. changing the used packages or package versions.

Grunt\footnote{https://gruntjs.com/}, a Node task runner module, is used for the actual development server configuration.
It enables defining tasks and using them to create custom development conditions and helpers.
Tasks can be defined in the required \textit{Gruntfile}.
While the predefined \textit{serve} task creates a local server on port 9000 with live reload, the \textit{build} task does optimizations to the code (concat, minify, uglify etc.) and then outputs these in a specified directory.
The latter is also used for production deployment.

For client sided libraries, that are utilized only in the application but not for the deployment, the Node module \textit{Bower}\footnote{https://bower.io/} is used.
Like the Node file package.json, bower has it own package management file, called \textit{bower.json}.
There the used libraries for the app are listed and can be managed easily.

AngularJs is used as JavaScript framework and allows the implementation of single-page web applications to create a user experience similar to desktop apps.
Furthermore, its specific HTML attributes, like \textit{ng-if}, \textit{ng-repeat} or \textit{ng-class}, and its \textit{two-way-binding} principle eliminate the need for manual binding of variables and synchronization between HTML and JavaScript files.
The ability of AngularJs to create reuseable modules, called directives, has a big impact on removing duplicate code, thus improving the overall code structure.

\subsection{Content Area}
The content area contains all the video, image and audio resources that are used in the app for stitching and creating new videos.
It is further divided into two components: A small menu and the actual pool of imported media.
New media sources can be added to the pool using the input field of the content area menu with a publicly accessible URL from any cloud provider or host\footnote{see README of GitHub repository}.
Via HTML Audio/Video events, meta information (thumbnails, type or file length) will be loaded and added asynchronously to the object representating the resource that we call content (material) from now on.
For a better distinction, the middle frame of each video is used as thumbnail.
Moreover, an icon on the lower right corner of the content also indicates the type of a content material.
Using the mouse, materials can be dragged around to add them to the timeline area or to delete them.
By double-clicking, individual content can be previewed in a dedicated pop-up player.
Also, buttons are provided on the menu to save the state of whole working sessions and to recall them at a later time.

For displaying material in the content area the \textit{contentList} object is used and bonded to the view.
Each time that object is updated, the content area view will be refreshed, too.
ContentList is a simple JavaScript dictionary object which consist of a random generated \textit{contentId} as key and an instance of the JavaScript class \textit{Content} as a value.
The dictionary data type was chosen to enable fast and direct access to the objects of the contentList requiring only the contentId, which can then be passed throughout the application if needed elsewhere.

A Content object consist of a name, a type, a length, an url, an mpd and an active property.
The name property is empty at the beginning and the user can change the name after the content material is loaded.
The url property is the publicly accessible URL of the video, audio or image that is used to add a material and can not be changed afterwards.
Each material URL serves as unique identifier in the contentList so that duplicated material URLs can not be added.
As a result, a duplicated content material can still be added to the list by using by using different URLs of the same content.
The active property of a material is used to keep track of the amounts of that material referenced on the timeline.
Each material added to the timeline increases the active counter and each one removed from the timeline decreases the counter.
Unless the active property equals 0 it is not possible to delete the content material from the pool.
Last but not least, the mpd property is not editable by the user. It is used to temporarily store the MPD URL after the segmentation process of the FOKUS backend and then used for the stitching request.
\begin{figure}[H]
\centering
\includegraphics[scale=0.46]{content_area.png}
\caption{Content area and menu bar with example resource materials.}
\end{figure}
The drag and drop functionality to add content materials to the timeline or delete content is enabled through the AngularJS module AngularHammer\footnote{https://github.com/RyanMullins/angular-hammer}.
The JavaScript library to recognize touch, mouse and pointer events HAMMER.JS\footnote{http://hammerjs.github.io/getting-started} was made into a AngularJS module for a simple usage.
The three most important callbacks of this module were the \textit{panStart}, \textit{panMove} and \textit{panEnd} events.
PanMove provides functionalities to recognise the mouse events and manipulates the DOM element while panStart and panEnd provide manupulation right before and after panMove is executed.
This way, it was possible to initialize needed helper variables (i.e. activate attributes to react while moving) and saving the current app state after moving elements.
Dragging materials will create a drag indicator, showing and representing the active drag action.
The indicator will disappear once the material is dropped and will execute specific actions depending on the dropping area.

A trash can icon appears in the content area whenever a material element is dragged around.
When the user drops a content into the bin, that material object is removed from the content area (provided its active property is 0).
Showing and hiding the trash is activated using panStart and panEnd.
Dropping the material into the timeline will create a new chunk on that area.

Another aspect of the content area menu is saving and loading working sessions.
Saving serializes and exports the current state of the content and timeline area while loading will recreate a working session from a saved file.
Based on the used data structure of the internal objects to be saved, the created simple text file contains one composed JSON object.
Because of the simplicity of the saved file and the absent of validation for loading sessions, a user might be able to input invalid data for the content and timeline area.

To preview individual contents, the user can double click on a specific material to open up a pop-up viewer/player showing images or playing videos and audios.
The double click is handled by AngularHammer and will create an overlay on the app.
Usually that overlay is hidden in the background but is brought to the front when needed.
Dinamically the necessary HTML element is created depending on the content material type to be previewed.
By clicking outside the overlay content, the content will be paused (only for audio and video player), destroyed and the overlay is hidden in the background again.

\subsection{Timeline Area}
Mainly the user will be working in the timeline area to create and edit the stitching video. Unlike the content area, the timeline area is devieded into three segments, the timeline bar where the materials are added as chunks, a menu to execute serveral functionalities and an information area to view the chunk data. Added chunks of the timeline can be focused and edited by dragging or by interaction with the menu buttons. The information area will show meta more detailed information of focused chunks.

Chunks are JavaScript objects with data about how the material is used in the timeline. The properties of chunks are objectListId, start, end, offset, mute and name. Naming chunks should help the user to easily distinguishing two different chunks of the same material. Names of chunks are the same from the created material and can be changed afterwards by the user. Mute allowes the user to turn of the sounds of a video or audio while playpack. The properties start and end are the time values when it starts and when it ends in the timeline. Offset indicates after which time of the original material the chunk starts to play, like if the user wants to start from the middle of the material. To connect between the chunks of the timeline and the material of the content, the objectListId is used. It has the contentId of the material and is used to get the originated material properties (length, url, type etc.) of the chunks, to avoid duplicated data in the chunks.

Added material chunks in the timeline are stored in a array, called timelineList, unlike the contentList dictionary object. Reason for that is to have a simple way to store the material chunks and having a ordered data structure. The amount of same chunks in the timeline is unlimited. Like the contentList object, the elements of the timelineList is used to generate a visual representation of the chunks. Due to the characteristic of overlap between video and audio content, the timeline is devided into two parts. Video and image chunks (no overlapping allowed) are positioned in one timeline part and audio chunks are positioned on a seperate timeline part and can be played at the same time like the video / image chunks.

The scaling on the timeline makes it easier to estimate the start and end position of chunks. This scaling is dynamically created depending to the total length of the created video content. An important variable to this aspect is the pixelPerSeconds value, on which the dynamic calculations are based on. Mainly it is used to calculate the pixel position of the mouse or chunks on the timeline to a timevalue and vice versa, because the drag events are working with pixel values, but the chunks requires time values. The visible timeline scales are basicly dynamic created DOM block elements, which positioned is calculated then by the pixelPerSeconds and the time amount of the scale step size. The menu buttons for zooming changes the pixelPerSeconds and the scale steps value and the two way binding mechanism of AngularJS updated the visual of the timeline according to the values.

Most of the chunk editing functionalities are chunks specific, so the chunks in the timeline can be focused by click (AngularHammer event used). A second click on the same and a click on another chunk will end the focus. A required focus for chunk editing is the deleting button. The click on this menu button only deletes the focused chunks, by removing the element from the array and does nothing, if no chunk is focused.

To rearange the time position of the chunks in the timeline, the user can drag the chunks and move them in the timeline. Like in the content area, this dragging feature is provided by AngularHammer. To only allow the chunks to move in a limited range, like the start and end of timeline or end of previous chunk and start of next chunk, the panStart event calculate this values before the panMove event. In the first version, the limits were used to check if panMove is allowed or not. Due to the delayed detection of fast mouse movement postions, the dragging was often stops right before the limited position and a space was presented in between. The optimization afterwards used the limits to detect of the chunk positon if within the allowd space and set it to the limit value, if these boundaries are exceeded. The panEnd event then calculates the pixel positon of the chunks to a time value and set them. To reduce the required mouse position precision by the user, the time value calculations will be quantized by 100 ms. This may reduce contents, but the chance of users recognising the difference is small and acceptable.

Like dragging the chunks within the free given space of the timeline, the shortening of the start and the end of chunks is executed by dragging the chunk modifier. These modifier are visible when the chunk is focused and not dragged. The limits for the modifier are, besides the limits of the chunk dragging limits, when the original start and end of the material. This additional limits prevents chunk offsets less than 0 and ends greater then length of the material. 

Originally it was planed to move chunks in the timeline and reordering the chunks by dragging them. To avoid accidential reordering while moving chunks and increase the UX, the reordering was lastly implemented by menu action buttons. Focused chunks can swapped the order with the previous or the next chunks, by reordering the element position within the timelineList array. 

Another timeline editing feature requiring chunk focus is the chunk cutting. This allows the user to cut one chunk by the specified time position into two seperate chunks, one ends and one starts by the cutted position. A modified chunk clone will be added in the timelineList after the the position of the cutted chunk. The clone and the cutted chunk values are then setted new. The chunk start of the cutted and the chunk end of the cloned chunks remain the same. The chunk end of the cutted and the chunk start of the cloned are set to the quantized cutting position. The offset of the clone musst then recalculated by the origingal offset adding the difference by the cut. 

If the user make a wrong actions in the timeline, the undo button in the menu helps them to jump back in the execution of timeline actions. There are ten chronologically ordered states of the timeline stored in the savedSteps array. The states are JSON converted string of the timelineList pushed into savedSteps after certain (panEnd, swap etc.) timeline action. The JSON string convertion is needen, because by adding only the timelineList into the array will only reference to the timelineList. So each added state reference to the timelineList and will not contain the chronological state changes but the current state of timelineList. To keep savedSteps to ten states, adding a eleventh state will remove the first state of the array. Undo allows the user to jump back up to ten steps and the redo button will jump foreward up to the current state. The savedStepPointer tracks the current position in the chronological array while moving for- and backward. \\

\subsection{Preview Player}
The preview player emulates the stitching result of the service backend without actually sending the necessary requests.
In doing so, imperfections in the stitching configurations can be detected and corrected via real-time feedback.
Users are not impelled to wait for the actual stitching response, thus saving them time.
\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{preview_player.png}
\caption{Look and controls of preview player.}
\end{figure}
\subsubsection{Preview Player - Features}
The implemented preview player supports all common controls of a usual browser media player.
These include: a ``play/pause''-button, a ``time-position''-slider and a ``volume''-slider.
On top of that, a ``restart''-button, a ``loop-play''-toggle and a ``play-in-range''-slider were added.
For the latter the \textit{noUiSlider}\footnote{https://refreshless.com/nouislider/} library is utilized.
Functionally, the ``play-in-range''-slider marks a time range in which the ``time-position''-slider is contraint.
Almost all of the listed controls can be operated via shortkeys\footnote{see README of GitHub repository} making the web UI behave more like a desktop application.
\\
\subsubsection{Preview Playback - Logic}
For audio and image preview playback it is sufficient to only have one HTML audio, respectively image, element and change its sources in real-time while playing.
However, this method does not work for all video sources, e.g. cloud provider URLs generally take more time for the initial loading which leads to stuttering, especially right after swapping video sources.
Therefore, per video chunk the implemented player creates one dedicated HTML video element with preloaded source.
While playing, it then only manipulates the z-indexes of the video elements and stops or starts them accordingly (see Algorithm 1) resulting in a smoother playback and minimized buffering time.

As optimization, whenever a new video chunk is added to the timeline area, the preview player is signalled to inspect whether there already exists an HTML video element referencing the same source.
If there is none, then one will be created.
Analogously, if a video chunk is removed from the timeline, then its corresponding HTML video element is deleted if there is no other chunk requiring the same source.
\begin{algorithm}[H]
\caption{Preview playback loop, simplified}
	\begin{algorithmic}[1]
		\STATE cc $\gets$ current chunk
		\STATE pc $\gets$ previous chunk
		\IF {cc $\ne$ pc $\AND$ (pc.type $=$ video $\OR$ pc.type $=$ audio)}
		\STATE pcHTML $\gets$ corresponding HTML video/audio element
		\STATE pause pcHTML
		\ENDIF
		\IF {time $=$ timeline-end $\OR$ time $=$ range-end}
		\IF {loop play active}
		\STATE go to range-start
		\ELSE
		\STATE pause player
		\RETURN
		\ENDIF
		\ENDIF
		\STATE bring current video/image to the front via z-index
		\IF {cc.type $=$ video $\OR$ cc.type $=$ audio}
		\STATE ccHTML $\gets$ corresponding HTML video/audio element
		\STATE calculate ccHTML offset position
		\STATE mute ccHTML if necessary
		\STATE play ccHTML
		\ENDIF
		\STATE pc $\gets$ cc
		\STATE time $\gets$ time counter + 100 ms
		\STATE diff $\gets$ (real-time-passed - time)
		\STATE repeat Algorithm 1 in (100 ms - diff)
	\end{algorithmic}
\end{algorithm}
Due to the nature of Javascript, Algorithm 1 had to be seperated into two functions code-wise or else the parallel updated time counter would start one loop ahead of the preview player resulting in both units not being in sync.
To keep the time counter even more precise, a self-adjustment calculation was introduced. By tracking the real-world time passed and matching that with the internal player time, it can be ensured that the preview playback loop is almost always repeated accurately every 100 milliseconds.

\subsection{SQS}
Once the user is satisfied with the constructed timeline chunks, the whole stitching procedure can be initiated by pressing the ``stitching''-button.
The stitching process consists of two phases: 1.) A segmentation phase and 2.) a stitching phase.

For communicating, the stitching backend operates over two seperate SQS queues.
From the web client perspective, one queue is for sending requests, the other is needed for receiving results.
Both are used for segmentation as well as for stitching requests.
The request type is determined through the message body by the service backend.
Further, each message body possesses a user defined \textit{jobID} which helps users to identify their messages in the queue for responses.
As with other libraries utilized in the developed user interface, functionalities of Amazon's SQS were made accessible by incorporating the official AWS SDK\footnote{https://github.com/aws/aws-sdk-js\#using-bower} for JavaScript via Bower.
\begin{figure}[H]
\begin{lstlisting}
var sqs = new AWS.SQS({
  "accessKeyId": sqsAccessKeyId,
  "secretAccessKey": sqsSecretAccessKey,
  "region": sqsRegion
});
\end{lstlisting}
\caption{Setting up an SQS instance in AngularJS.}
\end{figure}
At the beginning, the condition of the timeline and content area at process starting time are temporarily preserved.
By working on those data, accidental user changes and edits in the meantime don't affect the stitching procedure.
To avoid further problems, it is not possible to send another stitching request until the whole process is completed.
\\
\subsubsection{Segmentation Phase}
In this context, segmentation means creating an MPD file based on an audio, image or video file, requiring only a publicly accessible resource URL.
Theoretically, since fragmentation is applied to files in full length and not single chunks, this step could be executed as soon as a source is added to the content area or when a content is added to the timeline.
However, the user has to wait for the stitching process either way and not every resource might be used in the end which would then lead to spamming and wasting storage of the FOKUS backend.
Also worth mentioning is that for segmenting images a length must be specified priorly which is only possible after actually working on the timeline.

During phase 1, chunks in the timeline are fragmented in chronological order.
A segmentation request message to a chunk is created and put on the request queue (see figure \ref{fig:segmentation}).
While images need an extra \textit{loop} attribute compared to videos to indicate their length, audio files require an \textit{onlyaudio} attribute, although the fragmentation of latter are currently not supported by the backend.
\begin{figure}[H]
\begin{lstlisting}
var msg = {
  jobID: segmentationId,
  S3URL: chunkUrl,
  encodingprofile: "default",
  requestEnqueueTime: +new Date()
};

var params = {
  MessageBody: JSON.stringify(msg),
  QueueUrl: requestQueueURL,
  MessageGroupId: 'myGroupId'
};

sqs.sendMessage(params, function(err, data) {
  if (err) {
    console.log('ERROR: ', err);
  } else {
    console.log("send request successfully");
  }
});
\end{lstlisting}
\caption{Example request for segmenting a video.}
\label{fig:segmentation}
\end{figure}
After the segmentation request was send, the result queue will be polled for via a recursive receiving function.
The reason for this lies in the AWS SDK, which only allows the client to receive at most the 10 oldest messages on the result queue.
This in turn means that if 10 or more messages of other users are occupying the queue first, then the client has to wait until those are removed by their target user. 

The implemented approach polls for the maximum number of 10 messages at a time and only considers those targeting the user via identifying the \textit{jobID}.
Are among those messages no result messages, a progress per chunk fragmentation is shown and the next 10 are polled for.
In case one segmentation is done, a message contains an MPD URL, which is saved temporarily and the next chunk will be segmented, but only if its corresponding source URL has not been segmented yet.
Either way, those messages are then removed from the queue by its proper recipient so that the next responses in the FIFO queue can be received.

By default, short polling is used in SQS.
Long polling though yields several benefits and is enabled by setting the \textit{WaitTimeSeconds} parameter when receiving messages from a queue.
The developed algorithm uses the maximum value of 20 seconds.
This way, the probability of getting empty replies is disposed of by waiting until messages are available before responding to the client.
Nevertheless, messages are still returned as soon as they become obtainable.
Long polling further queries all Amazon SQS servers instead of a mere subset so that false empty responses are avoided, too.
\\
\subsubsection{Stitching Phase}
Are all chunks segmented successfully, the stitching phase starts.
By sewing MPDs together according to a specified configuration, a new video MPD is generated.
Except the message to be transmitted, sending and receiving a stitching request are analog to that of a segmentation request.
\begin{figure}[H]
\begin{lstlisting}
var configStitching = {
  "LOG_LEVEL": "info",
  "server": {
    "url": "http://localhost:8090",
    "staticFolder": "adstitcher-srv/public/mpds",
    "dashEndpoint": "/Users/fr/Documents/adstitcher-srv/public/mpds/"
  },
  "content": [
    {
      "type":"video",
      "begin":0.8,
      "end":5.5,
      "offset":0,
      "mute":false,
      "hide":false,
      "url":"https://s3.eu-central-1.amazonaws.com/wesualizesegment/exampleVideo1/Manifest.mpd"
    },
    {
      "type":"video",
      "begin":6.3,
      "end":10.0,
      "offset":3.7,
      "mute":false,
      "hide":false,
      "url":"https://s3.eu-central-1.amazonaws.com/wesualizesegment/exampleVideo2/Manifest.mpd"
    }
  ]
}

var msg = {
  jobID: stitchingId,
  config: configStitching,
  requestEnqueueTime: +new Date()
};
\end{lstlisting}
\caption{Example message of a stitching request}
\end{figure}
While the segmentation works perfectly, the stitching request results in an error (\textit{errorCode 11: "something wrong with the dashEndpoint in the config. Please check the input config."}).
Since the \textit{dashEndpoint} information was provided to us and there was no documentation of the stitching backend as well as the developers not always being contactable, this issue could not be resolved in this work.
\\
\subsubsection{Problems when receiving messages}
During tests it became apparent, that not every polling call resulted in the client receiving messages.
Despite that, when only one client is polling for the FIFO result queue at a time, it works perfectly.
As soon as more users attempt to receive messages, the whole process is slowed down massively even though long polling is used.
It might be that the concurrent pollings from different clients are interferring with each other so that the probability of receiving empty responses is increased rapidly.
This way, the FIFO queue can not make any or only very slow progresses.
On the other hand it might also be that the SQS queues are deployed on budget servers that are not able to actually handle multiple requests simultaneously.

Even worse, when many clients are polling for results, and some of these abort this process, then that might lead to stagnation of the result queue, whereby no active user can receive any more messages.

In this prototype version of the UI, a ``purge\&restart''-button was implemented as a safety measure for now.
This button will remove all messages on the queue and restart the FOKUS backend server, although this is no longterm solution to menttioned problems.

\section{Discussion}
In itself, the implemented user interface works without causing any problems.
The intuitive UI and the vast amount of implemented features, mostly inspired by professional video tools, offer a great variety of editing possibilities.
Most work related to SQS however, is not impeccable.
The stitching phase error aside, the approach that was taken to communicate with the FOKUS stitching backend still remains immature.
Whenever two or more clients are simultaneously waiting for responses from the queue, it takes much longer until results are transmitted.
First and foremost, it's due to Amazon only allowing to receive not more than 10 messages per query, favouring congestion of the result queue.
Above all, concurrent polling by many clients on the same queue severely increases the probability of receiving empty responses for whatever reasons.
Since this is the case, it makes sense to debate whether Amazon's Simple Queue Service is suited as interface between the implemented frontend and the existing backend.
We believe that the already stated potential benefits of using SQS outweigh the problems we encountered during this work.
Theoretically, if every long polling delivered non-empty responses, the implemented approach would work flawlessly because each client would quickly remove its own messages from the queue as soon as they are available.

One possible solution to this might be a dedicated intermediary backend (IB) managing the sending of requests and receiving of results.
Instead of direct interaction with the queues, each client will then have to use an API provided by the IB.
The intermediary backend remembers clients sending requests via its API and forwards these requests to the SQS request queue.
Then the IB alone extracts messages from the SQS result queue and forwards these messages to their actual targets.
Messages that can not be forwarded immediately are stored so they can be send again later.
After several retries and a timeout, that message will be deleted assuming its target client is either inactive or aborted the stitching process.
This way, the SQS result queue only needs to handle one polling instance.
On the downside, the intermediary backend would be required to cope with all the clients though.
How exactly this new issue is to be resolved, can not be decided right now.
Nevertheless, there should exist countless ways to implement such a backend.

\section{Conclusion}
In this work a prototype of a user interface offering many features and options for video and image stitching was implemented.
Video, audio and image content resources can be added to the application by simply using a publicly accessible URL.
On a timeline, chunks can be constructed to a new video by utilizing different editing possibilities.
Real-time feedback is given via a dedicated preview player emulating the stitching results without doing actual video cutting or processing.
By serializing whole working sessions, its state can be shared or recalled conveniently at a later time.
One of the main purposes though, i.e. to make the UI interact with the existing stitching backend by Fraunhofer FOKUS, could be satisfied only partially.
Having one sole client communicate with the provided FIFO queues is working perfectly fine.
As soon as more users concurrently attempt to poll for answers on the result queue, that queue becomes a bottleneck even though a more beneficial long polling method is used.
The reason for that might be in the implemented receiving algorithm or in the load handling capability of the server deploying the queues.
Possible solutions involve approaches using an intermediary backend to take care of sending and receiving messages from the SQS queues.
Another issue we encountered is that while the segmentation phase delivers the expected MPDs, the stitching phase results in an error.
Since there was no documentation of the FOKUS backend and with its developers not always being contactable, this problem could not be resolved in our work as of now.
In spite of mentioned deficiencies, the implemented prototype still qualifies as a fully functional user interface and forms the basis for a frontend of the existing stitching backend by Fraunhofer FOKUS.

\subsubsection*{Future Work}
While the realized prototype provides many user interaction possibilities, there is still room for further valuable features and improvements.
One of the most handy additions would be to control the preview player time and play-in-range positions by utilizing the currently only indicating position markers on the timeline.
Also, the un-/redo features merely apply for timeline changes at the moment.
When executed, ``active-element''-updates on content resources through adding and deleting chunks won't be taken into account.
It is desired to monitor these changes and other operations done on the content area.
Furthermore, dragging new chunks to the timeline, moving or shortening chunks would be more appealing if tooltips are displayed which show time stamps during these actions to increase the user experience.
An option to set the quantization value in the timeline would be another useful feature, although very small values might need too much processing power.
Moreover, the URL acceptance check and resource type whitelist matching algorithms would greatly benefit from enhancements, too.
Loading saved session files might need correctness checks in a future revision as well, so that this point can not be exploited, providing more robustness and consistency.

Regarding the communication with SQS, besides fixing the stitching phase error, it is suggested to implement a dedicated intermediary backend to take care of sending and receiving messages (as mentioned in \textbf{V. Discussion}).
Otherwise, strategies must be devised, so that concurrently receiving clients won't result in a bottleneck.

\begin{thebibliography}{00}
\bibitem{Fox} Fox, A., et al. (2009). ``Above the clouds: A berkeley view of cloud computing''. \textit{Dept. Electrical Eng. and Comput. Sciences, University of California, Berkeley, Rep. UCB/EECS}, 28(13), 2009. % https://s3.amazonaws.com/academia.edu.documents/43018702/0c960525ae2cc30314000000.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1532948428&Signature=a4lBKw0YmZSz%2B2FVebs0r0Szrn8%3D&response-content-disposition=inline%3B%20filename%3DAbove_the_Clouds_A_Berkeley_View_of_Clou.pdf
\bibitem{Sodagar} Sodagar, I. (2011). ``The mpeg-dash standard for multimedia streaming over the internet''. \textit{IEEE MultiMedia}, (4), 62-67.
\bibitem{VS} Varia, J., \& Mathew, S. (2014). ``Overview of amazon web services''. \textit{Amazon Web Services}. % https://genome.nyumc.org/hpcf/w/images/9/9e/HPC_course.spring_2014.session_3.AWS_Overview.pdf
\end{thebibliography}

\end{document}
